{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises: Level 1\n",
    "Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of lines in ./data/Obama's_speech.txt is: 66.\n",
      "The number of lines in ./data/Michelle's_speech.txt is: 83.\n",
      "The number of lines in ./data/Trump's_speech.txt is: 48.\n",
      "The number of lines in ./data/Melina's_speech.txt is: 33.\n",
      "\n",
      "The number of words in ./data/Obama's_speech.txt is: 2435\n",
      "The number of words in ./data/Michelle's_speech.txt is: 2250\n",
      "The number of words in ./data/Trump's_speech.txt is: 1261\n",
      "The number of words in ./data/Melina's_speech.txt is: 1392\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def no_lines(files):\n",
    "    with open(files) as obs:\n",
    "        lines = obs.read().splitlines()\n",
    "        return (f'The number of lines in {files} is: {len(lines)}.')\n",
    "\n",
    "files = [\"./data/Obama's_speech.txt\", \"./data/Michelle's_speech.txt\",\n",
    "        \"./data/Trump's_speech.txt\", \"./data/Melina's_speech.txt\"]\n",
    "for i in files:      \n",
    "    print(no_lines(i))\n",
    "print()\n",
    "\n",
    "def no_words(files):\n",
    "    with open(files) as obs:\n",
    "        text = obs.read()\n",
    "        words = re.sub(r'[.,;\"-]', '', text)\n",
    "        words = re.sub(r'[:\\n]', ' ', words)\n",
    "        words = re.split(' ', words)\n",
    "        return (f'The number of words in {files} is: {len(words)}')\n",
    "\n",
    "files = [\"./data/Obama's_speech.txt\", \"./data/Michelle's_speech.txt\",\n",
    "        \"./data/Trump's_speech.txt\", \"./data/Melina's_speech.txt\"]\n",
    "for i in files:      \n",
    "    print(no_words(i))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(91, 'English'), (45, 'French'), (25, 'Arabic'), (24, 'Spanish'), (9, 'Portuguese'), (9, 'Russian'), (8, 'Dutch'), (7, 'German'), (5, 'Chinese'), (4, 'Serbian')]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def most_spoken_languages(file, num):\n",
    "    with open(file) as obj:\n",
    "        world = obj.read()\n",
    "        earth = json.loads(world)\n",
    "        language = list()\n",
    "        for i in range(len(earth)):\n",
    "            language.extend(earth[i]['languages'])\n",
    "        spoken = dict()\n",
    "        most_spoken = list()\n",
    "        for h in language:\n",
    "            spoken[h] = language.count(h)\n",
    "        _spoken = sorted(spoken.values(), reverse=True)\n",
    "        for i in _spoken:\n",
    "            for j in language:\n",
    "                if language.count(j) == i and (language.count(j), j) not in most_spoken:\n",
    "                    most_spoken.append((i, j))\n",
    "        return (most_spoken[ : num])       \n",
    "\n",
    "print(most_spoken_languages('./data/countries_data.json', 10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'country': 'China', 'population': 1377422166}, {'country': 'India', 'population': 1295210000}, {'country': 'United States of America', 'population': 323947000}, {'country': 'Indonesia', 'population': 258705000}, {'country': 'Brazil', 'population': 206135893}, {'country': 'Pakistan', 'population': 194125062}, {'country': 'Nigeria', 'population': 186988000}, {'country': 'Bangladesh', 'population': 161006790}, {'country': 'Russian Federation', 'population': 146599183}, {'country': 'Japan', 'population': 126960000}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def most_populated_countries(file, num):\n",
    "    with open(file) as obj:\n",
    "        world = obj.read()\n",
    "        earth = json.loads(world)\n",
    "        c_population = dict()\n",
    "        most_populated = []\n",
    "        for i in range(len(earth)):\n",
    "            c_population[earth[i]['name']] = earth[i]['population']\n",
    "        for j in sorted(c_population.values(), reverse=True):\n",
    "            for k in c_population.keys():\n",
    "                if c_population[k] == j:\n",
    "                    most_populated.append({'country' : k, 'population' : j})\n",
    "        return (most_populated[ : num])\n",
    "\n",
    "print(most_populated_countries('./data/countries_data.json', 10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent words in ./data/Obama's_speech.txt is: [(120, 'the'), (107, 'and'), (81, 'of'), (66, 'to'), (58, 'our'), (57, ''), (50, 'we'), (48, 'a'), (47, 'that'), (36, 'is')]\n",
      "The most frequent words in ./data/Michelle's_speech.txt is: [(84, 'to'), (80, 'and'), (78, 'the'), (47, ''), (46, 'of'), (41, 'â€”'), (41, 'a'), (40, 'that'), (36, 'in'), (27, 'our')]\n",
      "The most frequent words in ./data/Trump's_speech.txt is: [(61, 'the'), (53, 'and'), (40, 'will'), (38, 'of'), (32, 'to'), (30, 'our'), (26, 'we'), (20, 'is'), (16, 'America'), (15, 'We')]\n",
      "The most frequent words in ./data/Melina's_speech.txt is: [(73, 'and'), (54, 'to'), (48, 'the'), (29, 'is'), (28, 'I'), (27, 'for'), (25, 'of'), (22, 'a'), (19, 'that'), (18, 'you')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def find_most_frequent_words(file, num):\n",
    "    with open(file) as obj:\n",
    "        text = obj.read()\n",
    "        words = re.sub(r'[.,;\"-]', '', text)\n",
    "        words = re.sub(r'[:\\n]', ' ', words) \n",
    "        words = re.split(' ', words)\n",
    "        count = []\n",
    "        for i in words:\n",
    "            if (words.count(i), i) not in count:\n",
    "                count.append((words.count(i), i))\n",
    "        s_count = sorted(count, reverse=True)\n",
    "        return (f'The most frequent words in {file} is: {s_count[ : num]}')\n",
    "\n",
    "files = [\"./data/Obama's_speech.txt\", \"./data/Michelle's_speech.txt\",\n",
    "        \"./data/Trump's_speech.txt\", \"./data/Melina's_speech.txt\"]\n",
    "for i in files:\n",
    "    print(find_most_frequent_words(i, 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ead1b95f633dc9c51826328e1846203f51a198c6fb5f2884a80417ba131d4e82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
